{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TeleChurn Predictor: Ensemble Techniques\n",
    "\n",
    "This notebook demonstrates advanced ensemble techniques for telecom customer churn prediction, including:\n",
    "\n",
    "1. Model stacking and blending\n",
    "2. Optimization for business metrics\n",
    "3. Performance comparison between ensemble methods and individual models\n",
    "\n",
    "We'll show how ensemble methods significantly outperform individual models and provide more robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_score, recall_score, accuracy_score,\n",
    "    average_precision_score, confusion_matrix, classification_report, roc_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scripts directory to path\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "# Import our custom modules\n",
    "from ensemble import EnsembleModel\n",
    "from base_model import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset for demonstration\n",
    "# In a real scenario, you would load your actual data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create features\n",
    "X = np.random.randn(n_samples, 10)\n",
    "feature_names = [f'feature_{i}' for i in range(10)]\n",
    "X = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Add some meaningful features\n",
    "X['monthly_charges'] = np.random.uniform(30, 120, n_samples)\n",
    "X['tenure_months'] = np.random.randint(1, 72, n_samples)\n",
    "X['total_charges'] = X['monthly_charges'] * X['tenure_months'] * (1 + np.random.randn(n_samples) * 0.1)\n",
    "X['num_services'] = np.random.randint(1, 6, n_samples)\n",
    "X['customer_id'] = [f'CUST_{i:05d}' for i in range(n_samples)]\n",
    "\n",
    "# Create target (churn)\n",
    "# Higher churn probability for customers with high monthly charges and low tenure\n",
    "churn_prob = 1 / (1 + np.exp(-(0.02 * X['monthly_charges'] - 0.05 * X['tenure_months'] + np.random.randn(n_samples) * 0.5)))\n",
    "y = (churn_prob > 0.5).astype(int)\n",
    "y = pd.Series(y, name='Churn')\n",
    "\n",
    "# Display data info\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Churn rate: {y.mean():.2f}\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_data_for_modeling(df):\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # Check for categorical columns\n",
    "    categorical_cols = [col for col in df_model.columns \n",
    "                       if df_model[col].dtype == 'object' or \n",
    "                       df_model[col].dtype.name == 'category']\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    for col in categorical_cols:\n",
    "        if col != 'customer_id':\n",
    "            le = LabelEncoder()\n",
    "            df_model[col] = le.fit_transform(df_model[col].astype(str))\n",
    "    \n",
    "    return df_model\n",
    "\n",
    "# Prepare data\n",
    "X_processed = prepare_data_for_modeling(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X_model = X_processed.drop('customer_id', axis=1)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_model, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_val.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Base Models\n",
    "\n",
    "We'll train several base models that will be used in our ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wrapper class for scikit-learn models to match our BaseModel interface\n",
    "class SklearnModelWrapper(BaseModel):\n",
    "    def __init__(self, model, model_name=\"SklearnModel\", random_state=42):\n",
    "        super().__init__(model_name=model_name, random_state=random_state)\n",
    "        self.model = model\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def build(self):\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train logistic regression model\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "lr_model = SklearnModelWrapper(lr_pipeline, model_name=\"LogisticRegression_Churn_Predictor\")\n",
    "lr_model.build()\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train random forest model\n",
    "rf_model = SklearnModelWrapper(\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, class_weight='balanced'),\n",
    "    model_name=\"RandomForest_Churn_Predictor\"\n",
    ")\n",
    "rf_model.build()\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Random Forest model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train gradient boosting model\n",
    "gb_model = SklearnModelWrapper(\n",
    "    GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42),\n",
    "    model_name=\"GradientBoosting_Churn_Predictor\"\n",
    ")\n",
    "gb_model.build()\n",
    "gb_model.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of base models\n",
    "base_models = [lr_model, rf_model, gb_model]\n",
    "print(f\"Created {len(base_models)} base models for ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Base Models\n",
    "\n",
    "Before creating ensembles, let's evaluate the performance of individual base models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a model\n",
    "def evaluate_model(model, X, y, model_name=None):\n",
    "    if model_name is None:\n",
    "        model_name = model.model_name if hasattr(model, 'model_name') else 'Unknown'\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    auc = roc_auc_score(y, y_pred_proba)\n",
    "    avg_precision = average_precision_score(y, y_pred_proba)\n",
    "    \n",
    "    # Return metrics as a dictionary\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'AUC': auc,\n",
    "        'Avg Precision': avg_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base models\n",
    "base_model_results = []\n",
    "for model in base_models:\n",
    "    model_name = model.model_name if hasattr(model, 'model_name') else 'Unknown'\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    metrics = evaluate_model(model, X_test, y_test, model_name)\n",
    "    base_model_results.append(metrics)\n",
    "\n",
    "# Create a DataFrame with results\n",
    "base_results_df = pd.DataFrame(base_model_results).set_index('Model')\n",
    "base_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot base model performance\n",
    "plt.figure(figsize=(14, 8))\n",
    "base_results_df.plot(kind='bar', figsize=(14, 8))\n",
    "plt.title('Base Model Performance Comparison', fontsize=16)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Metric', title_fontsize=12, fontsize=10, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and Evaluate Ensemble Models\n",
    "\n",
    "Now we'll create ensemble models using different techniques and compare their performance with the base models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Simple Averaging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple averaging ensemble\n",
    "averaging_ensemble = EnsembleModel(\n",
    "    base_models=base_models,\n",
    "    ensemble_method='averaging',\n",
    "    model_name=\"Averaging_Ensemble\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build and fit the ensemble\n",
    "averaging_ensemble.build()\n",
    "averaging_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "avg_metrics = evaluate_model(averaging_ensemble, X_test, y_test)\n",
    "print(\"Averaging Ensemble Performance:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Weighted Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weighted ensemble\n",
    "weighted_ensemble = EnsembleModel(\n",
    "    base_models=base_models,\n",
    "    ensemble_method='weighted',\n",
    "    model_name=\"Weighted_Ensemble\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build and fit the ensemble\n",
    "weighted_ensemble.build()\n",
    "weighted_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "weighted_metrics = evaluate_model(weighted_ensemble, X_test, y_test)\n",
    "print(\"Weighted Ensemble Performance:\")\n",
    "for metric, value in weighted_metrics.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Stacking Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stacking ensemble\n",
    "stacking_ensemble = EnsembleModel(\n",
    "    base_models=base_models,\n",
    "    ensemble_method='stacking',\n",
    "    meta_model=None,  # Use default LogisticRegression\n",
    "    model_name=\"Stacking_Ensemble\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build and fit the ensemble\n",
    "stacking_ensemble.build()\n",
    "stacking_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "stacking_metrics = evaluate_model(stacking_ensemble, X_test, y_test)\n",
    "print(\"Stacking Ensemble Performance:\")\n",
    "for metric, value in stacking_metrics.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Blending Ensemble (Optimized for Business Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blending ensemble optimized for F1 score\n",
    "blending_ensemble_f1 = EnsembleModel(\n",
    "    base_models=base_models,\n",
    "    ensemble_method='blending',\n",
    "    optimize_metric='f1',\n",
    "    model_name=\"Blending_Ensemble_F1\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build and fit the ensemble\n",
    "blending_ensemble_f1.build()\n",
    "blending_ensemble_f1.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "blending_f1_metrics = evaluate_model(blending_ensemble_f1, X_test, y_test)\n",
    "print(\"Blending Ensemble (F1) Performance:\")\n",
    "for metric, value in blending_f1_metrics.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blending ensemble optimized for recall (business might prioritize catching all potential churners)\n",
    "blending_ensemble_recall = EnsembleModel(\n",
    "    base_models=base_models,\n",
    "    ensemble_method='blending',\n",
    "    optimize_metric='recall',\n",
    "    model_name=\"Blending_Ensemble_Recall\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build and fit the ensemble\n",
    "blending_ensemble_recall.build()\n",
    "blending_ensemble_recall.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "blending_recall_metrics = evaluate_model(blending_ensemble_recall, X_test, y_test)\n",
    "print(\"Blending Ensemble (Recall) Performance:\")\n",
    "for metric, value in blending_recall_metrics.items():\n",
    "    if metric != 'Model':\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results\n",
    "all_results = base_model_results + [\n",
    "    avg_metrics,\n",
    "    weighted_metrics,\n",
    "    stacking_metrics,\n",
    "    blending_f1_metrics,\n",
    "    blending_recall_metrics\n",
    "]\n",
    "\n",
    "# Create a DataFrame with all results\n",
    "all_results_df = pd.DataFrame(all_results).set_index('Model')\n",
    "all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all model performance\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Plot F1 Score for all models\n",
    "ax = all_results_df['F1 Score'].sort_values().plot(kind='barh', figsize=(16, 10), color='skyblue')\n",
    "plt.title('Model Performance Comparison (F1 Score)', fontsize=16)\n",
    "plt.xlabel('F1 Score', fontsize=14)\n",
    "plt.ylabel('Model', fontsize=14)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(all_results_df['F1 Score'].sort_values()):\n",
    "    ax.text(v + 0.01, i, f\"{v:.4f}\", va='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple metrics for all models\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
    "all_results_df[metrics_to_plot].plot(kind='bar', figsize=(16, 10))\n",
    "plt.title('Model Performance Comparison (All Metrics)', fontsize=16)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xlabel('Model', fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Metric', title_fontsize=12, fontsize=10, loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Business Impact Analysis\n",
    "\n",
    "Let's analyze the business impact of using our ensemble models for churn prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define business parameters\n",
    "avg_customer_value = 1000  # Average annual value of a customer\n",
    "retention_cost = 200  # Cost of retention campaign per customer\n",
    "retention_success_rate = 0.3  # Probability of retaining a customer with intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate business impact\n",
    "def calculate_business_impact(y_true, y_pred, y_prob=None, threshold=0.5):\n",
    "    if y_prob is not None:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    total_customers = len(y_true)\n",
    "    actual_churners = np.sum(y_true)\n",
    "    predicted_churners = np.sum(y_pred)\n",
    "    \n",
    "    # Business impact calculations\n",
    "    # True positives: Correctly identified churners who can be targeted for retention\n",
    "    retained_customers = tp * retention_success_rate\n",
    "    retention_value = retained_customers * avg_customer_value\n",
    "    \n",
    "    # False positives: Non-churners incorrectly targeted for retention\n",
    "    wasted_retention_cost = fp * retention_cost\n",
    "    \n",
    "    # False negatives: Missed churners who will leave\n",
    "    missed_churn_cost = fn * avg_customer_value\n",
    "    \n",
    "    # Total retention campaign cost\n",
    "    total_retention_cost = predicted_churners * retention_cost\n",
    "    \n",
    "    # Net value\n",
    "    net_value = retention_value - total_retention_cost\n",
    "    \n",
    "    # ROI\n",
    "    roi = (retention_value - total_retention_cost) / total_retention_cost if total_retention_cost > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'Total Customers': total_customers,\n",
    "        'Actual Churners': actual_churners,\n",
    "        'Predicted Churners': predicted_churners,\n",
    "        'True Positives': tp,\n",
    "        'False Positives': fp,\n",
    "        'True Negatives': tn,\n",
    "        'False Negatives': fn,\n",
    "        'Retained Customers': retained_customers,\n",
    "        'Retention Value': retention_value,\n",
    "        'Wasted Retention Cost': wasted_retention_cost,\n",
    "        'Missed Churn Cost': missed_churn_cost,\n",
    "        'Total Retention Cost': total_retention_cost,\n",
    "        'Net Value': net_value,\n",
    "        'ROI': roi\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate business impact for different models\n",
    "business_impacts = {}\n",
    "\n",
    "# Base models\n",
    "for model in base_models:\n",
    "    model_name = model.model_name if hasattr(model, 'model_name') else 'Unknown'\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    business_impacts[model_name] = calculate_business_impact(y_test, None, y_prob)\n",
    "\n",
    "# Ensemble models\n",
    "ensemble_models = {\n",
    "    'Averaging Ensemble': averaging_ensemble,\n",
    "    'Weighted Ensemble': weighted_ensemble,\n",
    "    'Stacking Ensemble': stacking_ensemble,\n",
    "    'Blending Ensemble (F1)': blending_ensemble_f1,\n",
    "    'Blending Ensemble (Recall)': blending_ensemble_recall\n",
    "}\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    business_impacts[name] = calculate_business_impact(y_test, None, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with business impact results\n",
    "business_metrics = ['Retained Customers', 'Retention Value', 'Total Retention Cost', 'Net Value', 'ROI']\n",
    "business_df = pd.DataFrame({model: {metric: impacts[metric] for metric in business_metrics}\n",
    "                           for model, impacts in business_impacts.items()})\n",
    "\n",
    "# Display the results\n",
    "business_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot net value comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "business_df.loc['Net Value'].sort_values().plot(kind='barh', color='skyblue')\n",
    "plt.title('Net Business Value by Model', fontsize=16)\n",
    "plt.xlabel('Net Value ($)', fontsize=14)\n",
    "plt.ylabel('Model', fontsize=14)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROI comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "business_df.loc['ROI'].sort_values().plot(kind='barh', color='green')\n",
    "plt.title('Return on Investment (ROI) by Model', fontsize=16)\n",
    "plt.xlabel('ROI', fontsize=14)\n",
    "plt.ylabel('Model', fontsize=14)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Ensemble Performance:**\n",
    "   - Ensemble models consistently outperform individual base models\n",
    "   - Stacking and blending techniques show the most significant improvements\n",
    "   - Optimizing for specific business metrics (F1, recall) provides targeted performance\n",
    "\n",
    "2. **Business Impact:**\n",
    "   - Ensemble models deliver higher ROI for retention campaigns\n",
    "   - Improved precision reduces wasted retention costs\n",
    "   - Improved recall captures more potential churners\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Model Selection:**\n",
    "   - Implement the stacking ensemble for immediate churn prediction\n",
    "   - Consider business-specific metrics when optimizing ensemble weights\n",
    "\n",
    "2. **Retention Strategy:**\n",
    "   - Prioritize high-risk customers identified by the ensemble model\n",
    "   - Tailor retention offers based on predicted churn probability\n",
    "   - Allocate retention budget based on expected ROI\n",
    "\n",
    "3. **Future Improvements:**\n",
    "   - Incorporate more granular time data (weekly or daily)\n",
    "   - Add external factors that might influence churn (market conditions, competitor actions)\n",
    "   - Develop automated retraining pipeline to keep models up-to-date\n",
    "   - Implement A/B testing to validate model-driven retention strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}