{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TeleChurn Predictor: Model Training and Evaluation\n",
    "\n",
    "This notebook demonstrates the training and evaluation process for telecom customer churn prediction using our custom model modules. We'll train and compare different models with proper handling of class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scripts directory to path\n",
    "sys.path.insert(0, '../scripts')  # This ensures scripts directory is first in path\n",
    "\n",
    "# Import our custom modules\n",
    "from base_model import BaseModel, ModelFactory\n",
    "from gradient_boosting import GradientBoostingModel\n",
    "from neural_network import NeuralNetworkModel\n",
    "from training_pipeline import ModelTrainer, compare_models\n",
    "from feature_engineering import FeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data\n",
    "base_dir = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "processed_data_dir = os.path.join(base_dir, \"data\", \"processed\")\n",
    "train_file = \"preprocessed_cell2celltrain.csv\"\n",
    "holdout_file = \"preprocessed_cell2cellholdout.csv\"\n",
    "\n",
    "train_data = pd.read_csv(os.path.join(processed_data_dir, train_file))\n",
    "holdout_data = pd.read_csv(os.path.join(processed_data_dir, holdout_file))\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Holdout data shape: {holdout_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the training data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Churn to numeric if it's a string\n",
    "if train_data['Churn'].dtype == 'object':\n",
    "    print(\"Converting Churn from string to numeric...\")\n",
    "    # Map 'Yes'/'No' to 1/0\n",
    "    train_data['Churn'] = train_data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "if 'Churn' in holdout_data.columns and holdout_data['Churn'].dtype == 'object':\n",
    "    holdout_data['Churn'] = holdout_data['Churn'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the feature engineer\n",
    "feature_eng = FeatureEngineer(\n",
    "    remove_correlated=True, \n",
    "    correlation_threshold=0.85,\n",
    "    id_columns=['CustomerID'],  # Explicitly exclude CustomerID from feature engineering\n",
    "    selection_method='model_based'  # Use model-based feature selection for better results\n",
    ")\n",
    "\n",
    "# Apply feature engineering to training data\n",
    "train_featured = feature_eng.fit_transform(train_data.copy())\n",
    "\n",
    "# Apply feature engineering to holdout data\n",
    "holdout_featured = feature_eng.transform(holdout_data.copy())\n",
    "\n",
    "# Print shape comparison\n",
    "print(f\"Original training data shape: {train_data.shape}\")\n",
    "print(f\"Engineered training data shape: {train_featured.shape}\")\n",
    "print(f\"\\nOriginal holdout data shape: {holdout_data.shape}\")\n",
    "print(f\"Engineered holdout data shape: {holdout_featured.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_data_for_modeling(df):\n",
    "    df_model = df.copy()\n",
    "    \n",
    "    # Check for categorical columns\n",
    "    categorical_cols = [col for col in df_model.columns \n",
    "                       if df_model[col].dtype == 'object' or \n",
    "                       df_model[col].dtype.name == 'category']\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    for col in categorical_cols:\n",
    "        if col != 'Churn':\n",
    "            le = LabelEncoder()\n",
    "            df_model[col] = le.fit_transform(df_model[col].astype(str))\n",
    "    \n",
    "    # Ensure target is binary numeric\n",
    "    if 'Churn' in df_model.columns and df_model['Churn'].dtype == 'object':\n",
    "        df_model['Churn'] = df_model['Churn'].map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    return df_model\n",
    "\n",
    "# Prepare data\n",
    "train_featured_model = prepare_data_for_modeling(train_featured)\n",
    "holdout_featured_model = prepare_data_for_modeling(holdout_featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X = train_featured_model.drop('Churn', axis=1) if 'Churn' in train_featured_model.columns else train_featured_model\n",
    "y = train_featured_model['Churn'] if 'Churn' in train_featured_model.columns else None\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Prepare holdout set\n",
    "X_holdout = holdout_featured_model.drop('Churn', axis=1) if 'Churn' in holdout_featured_model.columns else holdout_featured_model\n",
    "y_holdout = holdout_featured_model['Churn'] if 'Churn' in holdout_featured_model.columns else None\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Holdout set: {X_holdout.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_val.value_counts(normalize=True) * 100)\n",
    "\n",
    "if y_holdout is not None:\n",
    "    print(\"\\nClass distribution in holdout set:\")\n",
    "    print(y_holdout.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation\n",
    "\n",
    "We'll train and evaluate multiple models using our custom modules:\n",
    "1. Gradient Boosting with XGBoost\n",
    "2. Gradient Boosting with LightGBM\n",
    "3. Neural Network\n",
    "\n",
    "Each model will be trained with proper handling of class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Gradient Boosting with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost model\n",
    "xgb_model = GradientBoostingModel(\n",
    "    model_name=\"XGBoost_Churn_Predictor\",\n",
    "    implementation=\"xgboost\",\n",
    "    params={\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 5,\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'scale_pos_weight': 1,  # Will be adjusted automatically for class imbalance\n",
    "        'random_state': 42\n",
    "    },\n",
    "    class_weight=\"balanced\",  # Handle class imbalance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create model trainer with SMOTE resampling\n",
    "xgb_trainer = ModelTrainer(\n",
    "    model=xgb_model,\n",
    "    resampling_strategy=\"smote\",  # Use SMOTE to handle class imbalance\n",
    "    resampling_ratio=0.5,  # Target 1:2 ratio of minority to majority class\n",
    "    random_state=42,\n",
    "    model_dir=\"../models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training pipeline for XGBoost\n",
    "xgb_results = xgb_trainer.run_training_pipeline(\n",
    "    X_train, y_train,\n",
    "    tune_hyperparameters=False,  # We're using predefined hyperparameters\n",
    "    tune_threshold=True,  # Find optimal classification threshold\n",
    "    cross_validate=True,  # Perform cross-validation\n",
    "    cv=5,\n",
    "    save_model=True,\n",
    "    save_history=True,\n",
    "    plot_cm=True,\n",
    "    plot_roc=True,\n",
    "    plot_pr=True,\n",
    "    plot_prob_dist=True,\n",
    "    plot_importance=True,\n",
    "    importance_top_n=20,\n",
    "    threshold_metric='f1'  # Optimize threshold for F1 score\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Gradient Boosting with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LightGBM model\n",
    "lgb_model = GradientBoostingModel(\n",
    "    model_name=\"LightGBM_Churn_Predictor\",\n",
    "    implementation=\"lightgbm\",\n",
    "    params={\n",
    "        'n_estimators': 200,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 5,\n",
    "        'num_leaves': 31,\n",
    "        'min_child_samples': 20,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'is_unbalance': True,  # Handle class imbalance\n",
    "        'random_state': 42\n",
    "    },\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create model trainer with undersampling\n",
    "lgb_trainer = ModelTrainer(\n",
    "    model=lgb_model,\n",
    "    resampling_strategy=\"undersample\",  # Use undersampling to handle class imbalance\n",
    "    resampling_ratio=0.5,  # Target 1:2 ratio of minority to majority class\n",
    "    random_state=42,\n",
    "    model_dir=\"../models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training pipeline for LightGBM\n",
    "lgb_results = lgb_trainer.run_training_pipeline(\n",
    "    X_train, y_train,\n",
    "    tune_hyperparameters=False,\n",
    "    tune_threshold=True,\n",
    "    cross_validate=True,\n",
    "    cv=5,\n",
    "    save_model=True,\n",
    "    save_history=True,\n",
    "    plot_cm=True,\n",
    "    plot_roc=True,\n",
    "    plot_pr=True,\n",
    "    plot_prob_dist=True,\n",
    "    plot_importance=True,\n",
    "    importance_top_n=20,\n",
    "    threshold_metric='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neural Network model\n",
    "nn_model = NeuralNetworkModel(\n",
    "    model_name=\"NeuralNetwork_Churn_Predictor\",\n",
    "    hidden_layers=[128, 64, 32],  # Three hidden layers\n",
    "    activations=\"relu\",\n",
    "    dropout_rate=0.3,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    class_weight=\"balanced\",  # Handle class imbalance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create model trainer with hybrid resampling\n",
    "nn_trainer = ModelTrainer(\n",
    "    model=nn_model,\n",
    "    resampling_strategy=\"hybrid\",  # Use hybrid approach (undersampling + SMOTE)\n",
    "    resampling_ratio=0.5,\n",
    "    random_state=42,\n",
    "    model_dir=\"../models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training pipeline for Neural Network\n",
    "nn_results = nn_trainer.run_training_pipeline(\n",
    "    X_train, y_train,\n",
    "    tune_hyperparameters=False,\n",
    "    tune_threshold=True,\n",
    "    cross_validate=True,\n",
    "    cv=5,\n",
    "    save_model=True,\n",
    "    save_history=True,\n",
    "    plot_cm=True,\n",
    "    plot_roc=True,\n",
    "    plot_pr=True,\n",
    "    plot_prob_dist=True,\n",
    "    plot_importance=True,  # Will use permutation importance\n",
    "    importance_top_n=20,\n",
    "    threshold_metric='f1',\n",
    "    train_params={\n",
    "        'validation_split': 0.2,\n",
    "        'verbose': 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for Neural Network\n",
    "nn_model.plot_training_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "trainers = [xgb_trainer, lgb_trainer, nn_trainer]\n",
    "comparison_df = compare_models(\n",
    "    trainers=trainers,\n",
    "    X=X_val,\n",
    "    y=y_val,\n",
    "    test_size=0.0,  # Use the entire validation set\n",
    "    metrics=['accuracy', 'precision', 'recall', 'f1', 'auc'],\n",
    "    plot=True,\n",
    "    figsize=(14, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison results\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Best Model on Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best model based on F1 score\n",
    "best_model_name = comparison_df.loc['f1'].idxmax()\n",
    "print(f\"Best model based on F1 score: {best_model_name}\")\n",
    "\n",
    "# Get the corresponding trainer\n",
    "if best_model_name == 'XGBoost_Churn_Predictor':\n",
    "    best_trainer = xgb_trainer\n",
    "elif best_model_name == 'LightGBM_Churn_Predictor':\n",
    "    best_trainer = lgb_trainer\n",
    "else:\n",
    "    best_trainer = nn_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the holdout set\n",
    "if y_holdout is not None:\n",
    "    # Get the optimal threshold from validation\n",
    "    optimal_threshold = best_trainer.training_history.get('optimal_threshold', {}).get('value', 0.5)\n",
    "    \n",
    "    print(f\"Using optimal threshold: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    # Evaluate on holdout set\n",
    "    holdout_metrics = best_trainer.evaluate_model(X_holdout, y_holdout, threshold=optimal_threshold)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    best_trainer.plot_confusion_matrix(X_holdout, y_holdout, threshold=optimal_threshold)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    best_trainer.plot_roc_curve(X_holdout, y_holdout)\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    best_trainer.plot_precision_recall_curve(X_holdout, y_holdout)\n",
    "else:\n",
    "    print(\"Holdout set does not have target labels for evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for the best model\n",
    "if hasattr(best_trainer.model, 'plot_feature_importance'):\n",
    "    importance_df = best_trainer.model.plot_feature_importance(top_n=20)\n",
    "    \n",
    "    # Display top features\n",
    "    print(\"\\nTop 20 features by importance:\")\n",
    "    display(importance_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning (Optional)\n",
    "\n",
    "For demonstration, we'll tune hyperparameters for the best model to potentially improve performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh instance of the best model for tuning\n",
    "if best_model_name == 'XGBoost_Churn_Predictor':\n",
    "    tuning_model = GradientBoostingModel(\n",
    "        model_name=\"XGBoost_Tuned\",\n",
    "        implementation=\"xgboost\",\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Define parameter grid for XGBoost\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    }\n",
    "    \n",
    "elif best_model_name == 'LightGBM_Churn_Predictor':\n",
    "    tuning_model = GradientBoostingModel(\n",
    "        model_name=\"LightGBM_Tuned\",\n",
    "        implementation=\"lightgbm\",\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Define parameter grid for LightGBM\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [15, 31, 63],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'min_child_samples': [10, 20, 30]\n",
    "    }\n",
    "    \n",
    "else:  # Neural Network\n",
    "    print(\"Hyperparameter tuning for Neural Network is more complex and time-consuming.\")\n",
    "    print(\"Skipping tuning for this demonstration.\")\n",
    "    tuning_model = None\n",
    "    param_grid = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters if we have a model to tune\n",
    "if tuning_model is not None and param_grid is not None:\n",
    "    # Build the model\n",
    "    tuning_model.build()\n",
    "    \n",
    "    # Tune hyperparameters\n",
    "    print(\"Tuning hyperparameters... (this may take a while)\")\n",
    "    tuning_model.tune_hyperparameters(\n",
    "        X_train, y_train,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,  # Use 3-fold CV for faster tuning\n",
    "        scoring='roc_auc',\n",
    "        n_iter=10,  # Try 10 random combinations\n",
    "        method='random'  # Use random search for faster tuning\n",
    "    )\n",
    "    \n",
    "    # Create trainer for tuned model\n",
    "    tuned_trainer = ModelTrainer(\n",
    "        model=tuning_model,\n",
    "        resampling_strategy=best_trainer.resampling_strategy,\n",
    "        resampling_ratio=best_trainer.resampling_ratio,\n",
    "        random_state=42,\n",
    "        model_dir=\"../models\"\n",
    "    )\n",
    "    \n",
    "    # Evaluate tuned model\n",
    "    tuned_metrics = tuned_trainer.evaluate_model(X_val, y_val)\n",
    "    \n",
    "    # Compare with best model\n",
    "    best_metrics = best_trainer.evaluate_model(X_val, y_val)\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    metrics_comparison = pd.DataFrame({\n",
    "        'Original Model': [best_metrics[m] for m in ['accuracy', 'precision', 'recall', 'f1', 'auc']],\n",
    "        'Tuned Model': [tuned_metrics[m] for m in ['accuracy', 'precision', 'recall', 'f1', 'auc']]\n",
    "    }, index=['accuracy', 'precision', 'recall', 'f1', 'auc'])\n",
    "    \n",
    "    # Display comparison\n",
    "    display(metrics_comparison)\n",
    "    \n",
    "    # Plot comparison\n",
    "    metrics_comparison.plot(kind='bar', figsize=(12, 8))\n",
    "    plt.title('Performance Comparison: Original vs Tuned Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Model Performance:**\n",
    "   - We trained and evaluated three different models: XGBoost, LightGBM, and Neural Network\n",
    "   - Each model was trained with proper handling of class imbalance using different strategies (SMOTE, undersampling, and hybrid approach)\n",
    "   - The best performing model was identified based on F1 score, which balances precision and recall\n",
    "\n",
    "2. **Class Imbalance Handling:**\n",
    "   - We demonstrated multiple approaches to handle class imbalance:\n",
    "     - Resampling techniques (SMOTE, undersampling, hybrid)\n",
    "     - Class weighting in the models\n",
    "     - Threshold optimization to balance precision and recall\n",
    "\n",
    "3. **Feature Importance:**\n",
    "   - We identified the most important features for churn prediction\n",
    "   - This provides actionable insights for business stakeholders\n",
    "\n",
    "4. **Hyperparameter Tuning:**\n",
    "   - We demonstrated how to tune hyperparameters to potentially improve model performance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Model Deployment:**\n",
    "   - Deploy the best model in a production environment\n",
    "   - Implement monitoring to track model performance over time\n",
    "\n",
    "2. **Feature Engineering Refinement:**\n",
    "   - Further refine feature engineering based on feature importance analysis\n",
    "   - Explore additional domain-specific features\n",
    "\n",
    "3. **Ensemble Methods:**\n",
    "   - Explore ensemble methods combining multiple models for potentially better performance\n",
    "\n",
    "4. **Explainability:**\n",
    "   - Implement SHAP values or other explainability techniques to provide more detailed insights into model predictions\n",
    "\n",
    "5. **Business Integration:**\n",
    "   - Develop a system to translate model predictions into actionable business interventions\n",
    "   - Create dashboards for business users to monitor churn risk and take preventive actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}