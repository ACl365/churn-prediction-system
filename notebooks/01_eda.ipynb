{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TeleChurn Predictor: Exploratory Data Analysis\n",
    "\n",
    "This notebook performs initial exploratory data analysis on telecom customer data for the TeleChurn Predictor project. The goal is to understand the data structure, identify patterns, and gain insights that will inform feature engineering and model development.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "TeleChurn Predictor is a machine learning system designed to predict customer churn in the telecommunications industry. This notebook is the first step in the data science pipeline, focusing on understanding the raw data before preprocessing and modeling.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Data Loading and Initial Inspection](#1.-Data-Loading-and-Initial-Inspection)\n",
    "2. [Summary Statistics and Data Types](#2.-Summary-Statistics-and-Data-Types)\n",
    "3. [Missing Value Analysis](#3.-Missing-Value-Analysis)\n",
    "4. [Distribution of Numerical Features](#4.-Distribution-of-Numerical-Features)\n",
    "5. [Analysis of Categorical Features](#5.-Analysis-of-Categorical-Features)\n",
    "6. [Target Variable Analysis](#6.-Target-Variable-Analysis)\n",
    "7. [Correlation Analysis](#7.-Correlation-Analysis)\n",
    "8. [Outlier Detection](#8.-Outlier-Detection)\n",
    "9. [Feature Importance Analysis](#9.-Feature-Importance-Analysis)\n",
    "10. [Conclusions and Next Steps](#10.-Conclusions-and-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (if not already installed)\n",
    "!pip install pandas numpy matplotlib seaborn scipy missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import missingno as msno\n",
    "import warnings\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection\n",
    "\n",
    "Let's load the telecom customer data and perform an initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "train_path = \"../data/raw/cell2celltrain.csv\"\n",
    "holdout_path = \"../data/raw/cell2cellholdout.csv\"\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(train_path)\n",
    "holdout_data = pd.read_csv(holdout_path)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Holdout data shape: {holdout_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the training data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the holdout data\n",
    "holdout_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows in training data\n",
    "print(f\"Number of duplicate rows in training data: {train_data.duplicated().sum()}\")\n",
    "\n",
    "# Check for duplicate CustomerIDs in training data\n",
    "print(f\"Number of duplicate CustomerIDs in training data: {train_data['CustomerID'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics and Data Types\n",
    "\n",
    "Let's examine the data types and summary statistics of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types\n",
    "train_data.dtypes.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "train_data.describe().T.sort_values(by='mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for categorical columns\n",
    "categorical_columns = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "for column in categorical_columns:\n",
    "    print(f\"\\n{column}:\")\n",
    "    print(train_data[column].value_counts(normalize=True).head(10) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorize features\n",
    "\n",
    "Let's categorize our features into different types for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features\n",
    "id_columns = ['CustomerID']\n",
    "target_column = ['Churn']\n",
    "\n",
    "# Categorical columns (non-binary, non-numeric)\n",
    "categorical_columns = [\n",
    "    'ServiceArea', 'CreditRating', 'PrizmCode', 'Occupation', 'MaritalStatus'\n",
    "]\n",
    "\n",
    "# Binary columns (yes/no or similar)\n",
    "binary_columns = [\n",
    "    'ChildrenInHH', 'HandsetRefurbished', 'HandsetWebCapable', 'TruckOwner',\n",
    "    'RVOwner', 'Homeownership', 'BuysViaMailOrder', 'RespondsToMailOffers',\n",
    "    'OptOutMailings', 'NonUSTravel', 'OwnsComputer', 'HasCreditCard',\n",
    "    'NewCellphoneUser', 'NotNewCellphoneUser', 'OwnsMotorcycle',\n",
    "    'MadeCallToRetentionTeam'\n",
    "]\n",
    "\n",
    "# Numerical columns (continuous)\n",
    "numerical_continuous_columns = [\n",
    "    'MonthlyRevenue', 'MonthlyMinutes', 'TotalRecurringCharge',\n",
    "    'DirectorAssistedCalls', 'OverageMinutes', 'RoamingCalls',\n",
    "    'PercChangeMinutes', 'PercChangeRevenues', 'DroppedCalls',\n",
    "    'BlockedCalls', 'UnansweredCalls', 'CustomerCareCalls',\n",
    "    'ThreewayCalls', 'ReceivedCalls', 'OutboundCalls', 'InboundCalls',\n",
    "    'PeakCallsInOut', 'OffPeakCallsInOut', 'DroppedBlockedCalls',\n",
    "    'CallForwardingCalls', 'CallWaitingCalls', 'AgeHH1', 'AgeHH2'\n",
    "]\n",
    "\n",
    "# Numerical columns (discrete/integer)\n",
    "numerical_discrete_columns = [\n",
    "    'MonthsInService', 'UniqueSubs', 'ActiveSubs', 'Handsets',\n",
    "    'HandsetModels', 'CurrentEquipmentDays', 'RetentionCalls',\n",
    "    'RetentionOffersAccepted', 'ReferralsMadeBySubscriber',\n",
    "    'IncomeGroup', 'AdjustmentsToCreditRating', 'HandsetPrice'\n",
    "]\n",
    "\n",
    "# All numerical columns\n",
    "numerical_columns = numerical_continuous_columns + numerical_discrete_columns\n",
    "\n",
    "# Print counts\n",
    "print(f\"Total features: {len(train_data.columns)}\")\n",
    "print(f\"ID columns: {len(id_columns)}\")\n",
    "print(f\"Target column: {len(target_column)}\")\n",
    "print(f\"Categorical columns: {len(categorical_columns)}\")\n",
    "print(f\"Binary columns: {len(binary_columns)}\")\n",
    "print(f\"Numerical continuous columns: {len(numerical_continuous_columns)}\")\n",
    "print(f\"Numerical discrete columns: {len(numerical_discrete_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Value Analysis\n",
    "\n",
    "Let's analyze missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing values\n",
    "missing = train_data.isnull().sum()\n",
    "missing_percent = missing / len(train_data) * 100\n",
    "\n",
    "# Create summary DataFrame\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Values': missing,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "\n",
    "# Sort by missing percentage\n",
    "missing_summary = missing_summary.sort_values('Missing Percentage', ascending=False)\n",
    "\n",
    "# Display columns with missing values\n",
    "missing_summary[missing_summary['Missing Values'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing value correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "msno.heatmap(train_data)\n",
    "plt.title('Missing Value Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution of Numerical Features\n",
    "\n",
    "Let's examine the distribution of numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot histograms for numerical features\n",
    "def plot_histograms(data, columns, bins=30, figsize=(16, 12), ncols=3):\n",
    "    nrows = int(np.ceil(len(columns) / ncols))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, column in enumerate(columns):\n",
    "        if i < len(axes):\n",
    "            sns.histplot(data[column], bins=bins, kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {column}')\n",
    "            axes[i].set_xlabel(column)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(len(columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for key numerical continuous features\n",
    "key_continuous_features = [\n",
    "    'MonthlyRevenue', 'MonthlyMinutes', 'TotalRecurringCharge',\n",
    "    'OverageMinutes', 'PercChangeMinutes', 'PercChangeRevenues',\n",
    "    'DroppedCalls', 'BlockedCalls', 'UnansweredCalls'\n",
    "]\n",
    "\n",
    "plot_histograms(train_data, key_continuous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for key numerical discrete features\n",
    "key_discrete_features = [\n",
    "    'MonthsInService', 'UniqueSubs', 'ActiveSubs', 'Handsets',\n",
    "    'HandsetModels', 'CurrentEquipmentDays', 'RetentionCalls',\n",
    "    'IncomeGroup', 'HandsetPrice'\n",
    "]\n",
    "\n",
    "plot_histograms(train_data, key_discrete_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots for numerical features\n",
    "\n",
    "Let's use box plots to identify potential outliers in our numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot box plots for numerical features\n",
    "def plot_boxplots(data, columns, figsize=(16, 12), ncols=3):\n",
    "    nrows = int(np.ceil(len(columns) / ncols))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, column in enumerate(columns):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(x=data[column], ax=axes[i])\n",
    "            axes[i].set_title(f'Box Plot of {column}')\n",
    "            axes[i].set_xlabel(column)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(len(columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for key numerical continuous features\n",
    "plot_boxplots(train_data, key_continuous_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis of Categorical Features\n",
    "\n",
    "Let's examine the distribution of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot bar charts for categorical features\n",
    "def plot_bar_charts(data, columns, figsize=(16, 12), ncols=2):\n",
    "    nrows = int(np.ceil(len(columns) / ncols))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, column in enumerate(columns):\n",
    "        if i < len(axes):\n",
    "            value_counts = data[column].value_counts().sort_values(ascending=False)\n",
    "            sns.barplot(x=value_counts.index, y=value_counts.values, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {column}')\n",
    "            axes[i].set_xlabel(column)\n",
    "            axes[i].set_ylabel('Count')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(len(columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar charts for categorical features\n",
    "plot_bar_charts(train_data, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar charts for binary features\n",
    "plot_bar_charts(train_data, binary_columns, ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target Variable Analysis\n",
    "\n",
    "Let's analyze the target variable (Churn) and its relationship with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the target variable\n",
    "churn_counts = train_data['Churn'].value_counts()\n",
    "churn_percent = train_data['Churn'].value_counts(normalize=True) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=churn_counts.index, y=churn_counts.values)\n",
    "plt.title('Distribution of Churn')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, p in enumerate(ax.patches):\n",
    "    ax.annotate(f\"{churn_percent.values[i]:.1f}%\", \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between numerical features and churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot numerical features by churn\n",
    "def plot_numerical_by_churn(data, columns, figsize=(16, 12), ncols=3):\n",
    "    nrows = int(np.ceil(len(columns) / ncols))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, column in enumerate(columns):\n",
    "        if i < len(axes):\n",
    "            sns.boxplot(x='Churn', y=column, data=data, ax=axes[i])\n",
    "            axes[i].set_title(f'{column} by Churn')\n",
    "            axes[i].set_xlabel('Churn')\n",
    "            axes[i].set_ylabel(column)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(len(columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot key numerical features by churn\n",
    "key_features_for_churn = [\n",
    "    'MonthlyRevenue', 'MonthlyMinutes', 'TotalRecurringCharge',\n",
    "    'OverageMinutes', 'PercChangeMinutes', 'PercChangeRevenues',\n",
    "    'DroppedCalls', 'BlockedCalls', 'UnansweredCalls',\n",
    "    'CustomerCareCalls', 'MonthsInService', 'RetentionCalls'\n",
    "]\n",
    "\n",
    "plot_numerical_by_churn(train_data, key_features_for_churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between categorical features and churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot categorical features by churn\n",
    "def plot_categorical_by_churn(data, columns, figsize=(16, 12), ncols=2):\n",
    "    nrows = int(np.ceil(len(columns) / ncols))\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, column in enumerate(columns):\n",
    "        if i < len(axes):\n",
    "            # Calculate churn rate by category\n",
    "            churn_rate = data.groupby(column)['Churn'].apply(lambda x: (x == 'Yes').mean() * 100).sort_values(ascending=False)\n",
    "            \n",
    "            # Plot\n",
    "            sns.barplot(x=churn_rate.index, y=churn_rate.values, ax=axes[i])\n",
    "            axes[i].set_title(f'Churn Rate by {column}')\n",
    "            axes[i].set_xlabel(column)\n",
    "            axes[i].set_ylabel('Churn Rate (%)')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add count labels\n",
    "            for j, p in enumerate(axes[i].patches):\n",
    "                category = churn_rate.index[j]\n",
    "                count = len(data[data[column] == category])\n",
    "                axes[i].annotate(f\"n={count}\", \n",
    "                            (p.get_x() + p.get_width() / 2., p.get_height() + 1), \n",
    "                            ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for j in range(len(columns), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot categorical features by churn\n",
    "plot_categorical_by_churn(train_data, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot binary features by churn\n",
    "plot_categorical_by_churn(train_data, binary_columns, ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation Analysis\n",
    "\n",
    "Let's analyze correlations between numerical features and with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data with binary target\n",
    "train_data_corr = train_data.copy()\n",
    "train_data_corr['Churn_Binary'] = train_data_corr['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Make sure we only include numeric columns\n",
    "numeric_cols = train_data_corr.select_dtypes(include=['number']).columns\n",
    "corr_data = train_data_corr[numeric_cols].copy()\n",
    "\n",
    "# Check for any remaining non-numeric values\n",
    "print(\"Checking for non-numeric values in correlation data:\")\n",
    "for col in corr_data.columns:\n",
    "    non_numeric = corr_data[col].map(lambda x: not np.issubdtype(type(x), np.number)).sum()\n",
    "    if non_numeric > 0:\n",
    "        print(f\"Column {col} has {non_numeric} non-numeric values\")\n",
    "        # Convert to numeric, coercing errors to NaN\n",
    "        corr_data[col] = pd.to_numeric(corr_data[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = corr_data.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top correlations with Churn\n",
    "if 'Churn_Binary' in corr_matrix.columns:\n",
    "    churn_corr = corr_matrix['Churn_Binary'].sort_values(ascending=False)\n",
    "    print(\"Top positive correlations with Churn:\")\n",
    "    print(churn_corr.head(10))\n",
    "    print(\"\\nTop negative correlations with Churn:\")\n",
    "    print(churn_corr.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top correlations with Churn\n",
    "if 'Churn_Binary' in corr_matrix.columns:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_corr = pd.concat([churn_corr.head(10), churn_corr.tail(10)])\n",
    "    top_corr = top_corr.drop('Churn_Binary')  # Remove self-correlation\n",
    "    sns.barplot(x=top_corr.values, y=top_corr.index)\n",
    "    plt.title('Top Correlations with Churn')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.axvline(x=0, color='black', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highly correlated features\n",
    "\n",
    "Let's identify pairs of highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get highly correlated pairs\n",
    "def get_highly_correlated_pairs(corr_matrix, threshold=0.7):\n",
    "    # Create a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Apply mask and get pairs with correlation above threshold\n",
    "    corr_pairs = corr_matrix.mask(mask).stack().reset_index()\n",
    "    corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "    \n",
    "    # Filter by threshold and sort\n",
    "    high_corr_pairs = corr_pairs[abs(corr_pairs['Correlation']) > threshold]\n",
    "    high_corr_pairs = high_corr_pairs.sort_values('Correlation', ascending=False)\n",
    "    \n",
    "    return high_corr_pairs\n",
    "\n",
    "# Get highly correlated pairs\n",
    "if 'Churn_Binary' in corr_matrix.columns:\n",
    "    high_corr_pairs = get_highly_correlated_pairs(corr_matrix.drop('Churn_Binary', axis=1).drop('Churn_Binary', axis=0))\n",
    "else:\n",
    "    high_corr_pairs = get_highly_correlated_pairs(corr_matrix)\n",
    "    \n",
    "high_corr_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Outlier Detection\n",
    "\n",
    "Let's identify outliers in our numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers_iqr(data, columns):\n",
    "    outlier_counts = {}\n",
    "    \n",
    "    for column in columns:\n",
    "        if column in data.columns and pd.api.types.is_numeric_dtype(data[column]):\n",
    "            Q1 = data[column].quantile(0.25)\n",
    "            Q3 = data[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "            outlier_counts[column] = len(outliers)\n",
    "    \n",
    "    return pd.Series(outlier_counts).sort_values(ascending=False)\n",
    "\n",
    "# Detect outliers in numerical features\n",
    "outlier_counts = detect_outliers_iqr(train_data, numerical_columns)\n",
    "outlier_percent = outlier_counts / len(train_data) * 100\n",
    "\n",
    "# Create summary DataFrame\n",
    "outlier_summary = pd.DataFrame({\n",
    "    'Outlier Count': outlier_counts,\n",
    "    'Outlier Percentage': outlier_percent\n",
    "})\n",
    "\n",
    "# Display features with outliers\n",
    "outlier_summary[outlier_summary['Outlier Count'] > 0].sort_values('Outlier Percentage', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outlier percentages\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_outliers = outlier_summary[outlier_summary['Outlier Count'] > 0].sort_values('Outlier Percentage', ascending=False).head(15)\n",
    "sns.barplot(x=top_outliers['Outlier Percentage'], y=top_outliers.index)\n",
    "plt.title('Features with Highest Percentage of Outliers')\n",
    "plt.xlabel('Outlier Percentage (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "\n",
    "Let's perform a preliminary feature importance analysis using a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a copy of the data\n",
    "model_data = train_data.copy()\n",
    "\n",
    "# Encode target variable\n",
    "model_data['Churn'] = model_data['Churn'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Check for non-numeric values in numerical columns\n",
    "for column in numerical_columns:\n",
    "    if column in model_data.columns:\n",
    "        # Convert to numeric, coercing errors to NaN\n",
    "        model_data[column] = pd.to_numeric(model_data[column], errors='coerce')\n",
    "\n",
    "# Handle missing values in numerical features\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "model_data[numerical_columns] = numerical_imputer.fit_transform(model_data[numerical_columns])\n",
    "\n",
    "# Encode categorical features\n",
    "for column in categorical_columns + binary_columns:\n",
    "    if column in model_data.columns:\n",
    "        le = LabelEncoder()\n",
    "        model_data[column] = le.fit_transform(model_data[column].astype(str))\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "model_data[numerical_columns] = scaler.fit_transform(model_data[numerical_columns])\n",
    "\n",
    "# Prepare features and target\n",
    "features = model_data.drop(['CustomerID', 'Churn'], axis=1)\n",
    "target = model_data['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(features, target)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': features.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "feature_importances.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Next Steps\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Target Variable Distribution:**\n",
    "   - The dataset has an imbalanced distribution of the target variable (Churn).\n",
    "   - [Add specific percentages based on actual data]\n",
    "\n",
    "2. **Missing Values:**\n",
    "   - [Summarize missing value findings]\n",
    "   - [Mention any patterns in missing data]\n",
    "\n",
    "3. **Feature Distributions:**\n",
    "   - Many numerical features have skewed distributions, suggesting the need for transformations.\n",
    "   - Several features contain outliers that may need special handling.\n",
    "\n",
    "4. **Correlations:**\n",
    "   - [Highlight key correlations with churn]\n",
    "   - [Mention highly correlated feature pairs that might cause multicollinearity]\n",
    "\n",
    "5. **Feature Importance:**\n",
    "   - [List top features identified by the Random Forest model]\n",
    "   - These features should be given special attention in the feature engineering phase.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Handle missing values using appropriate imputation techniques.\n",
    "   - Address outliers through capping, removal, or transformation.\n",
    "   - Apply feature scaling to normalize numerical features.\n",
    "\n",
    "2. **Feature Engineering:**\n",
    "   - Create interaction features between key variables.\n",
    "   - Develop ratio features (e.g., revenue per minute).\n",
    "   - Generate aggregated features for call patterns.\n",
    "   - Consider dimensionality reduction for highly correlated features.\n",
    "\n",
    "3. **Model Development:**\n",
    "   - Implement multiple models (Random Forest, XGBoost, Neural Networks).\n",
    "   - Address class imbalance through sampling techniques or class weights.\n",
    "   - Perform hyperparameter optimization.\n",
    "   - Evaluate models using appropriate metrics (AUC, F1-score, precision, recall).\n",
    "\n",
    "4. **Model Interpretation:**\n",
    "   - Use SHAP values to explain model predictions.\n",
    "   - Identify key factors driving churn.\n",
    "   - Develop actionable insights for business stakeholders.\n",
    "\n",
    "The next notebook in this series will focus on feature engineering based on the insights gained from this exploratory analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
